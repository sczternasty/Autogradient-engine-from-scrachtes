{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:14:42.462710Z",
     "start_time": "2025-04-09T17:14:40.884080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "from graphviz import Digraph"
   ],
   "id": "425c071ccc9df83",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def draw_computation_graph(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v.previous:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "\n",
    "  for n in nodes:\n",
    "    nid = str(id(n))\n",
    "    dot.node(name = nid, label = \"{ %s | %.4f | grad %.4f}\" % (n.label, n.x, n.grad), shape='record')\n",
    "    if n.operation:\n",
    "      dot.node(name = nid + n.operation, label = n.operation)\n",
    "      dot.edge(nid + n.operation, nid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2.operation)\n",
    "\n",
    "  return dot"
   ],
   "id": "456361c58d1ce1d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Var:\n",
    "    def __init__(self, x, children = (), operation='', label=''):\n",
    "        self.x = x\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self.previous = set(children)\n",
    "        self.operation = operation\n",
    "        self.label = label\n",
    "    def __repr__(self):\n",
    "         return str(self.x)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Var) else Var(other)\n",
    "        out =  Var(self.x + other.x, (self, other), \"+\")\n",
    "        def add_backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = add_backward\n",
    "        return out\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    def __rsub__(self, other):\n",
    "        return self - other\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Var) else Var(other)\n",
    "        out = Var(self.x * other.x, (self, other), \"*\")\n",
    "        def mul_backward():\n",
    "            self.grad += other.x * out.grad\n",
    "            other.grad += self.x * out.grad\n",
    "        out._backward = mul_backward\n",
    "        return out\n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    def tanh(self):\n",
    "        t = (math.exp(2*self.x) - 1)/(math.exp(2*self.x) + 1)\n",
    "        out = Var(t, (self, ), 'tanh')\n",
    "        def tanh_backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = tanh_backward\n",
    "        return out\n",
    "    def relu(self):\n",
    "        out = Var(0 if self.x < 0 else self.x, (self,), 'relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.x > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.x\n",
    "        out = Var(math.exp(x), (self,), 'exp')\n",
    "\n",
    "        def exp_backward():\n",
    "            self.grad += math.exp(x) * out.grad\n",
    "        out._backward = exp_backward\n",
    "        return out\n",
    "    def log(self):\n",
    "        x = self.x\n",
    "        out = Var(math.log(x))\n",
    "        def log_backward():\n",
    "            self.grad += (1/ x) * out.grad\n",
    "\n",
    "        out._backward = log_backward\n",
    "        return out\n",
    "    def __pow__(self, power):\n",
    "        assert isinstance(power, (int,float))\n",
    "        out = Var(self.x**power, (self,), f'**{power}')\n",
    "\n",
    "        def pow_backward():\n",
    "            self.grad += power * (self.x ** (power - 1)) * out.grad\n",
    "        out._backward = pow_backward\n",
    "        return out\n",
    "    def backward(self):\n",
    "        ordered = []\n",
    "        visited = set()\n",
    "        def order_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.previous:\n",
    "                    order_topo(child)\n",
    "                ordered.append(v)\n",
    "        order_topo(self)\n",
    "        self.grad = 1\n",
    "        for node in reversed(ordered):\n",
    "            node._backward()\n"
   ],
   "id": "f459c12d9592894f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "random.seed(42)\n",
    "\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.sequential = []\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "class Perceptron(Module):\n",
    "    def __init__(self, nin):\n",
    "        self.weight = [Var(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.bias = Var(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = sum((wi*xi for wi, xi in zip(self.weight, x)), self.bias)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weight + [self.bias]\n",
    "    def __repr__(self):\n",
    "        return f\"Neuron({len(self.weight)})\"\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.neurons = [Perceptron(in_features) for _ in range(out_features)]\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "class ReLu(Module):\n",
    "    def __call__(self, x):\n",
    "        return [i.relu() for i in x] if isinstance(x,list) else x.relu()\n",
    "    def __repr__(self):\n",
    "        return \"ReLu\"\n",
    "    def parameters(self):\n",
    "        return []\n",
    "class Tanh(Module):\n",
    "    def __call__(self, x):\n",
    "        return [i.tanh() for i in x] if isinstance(x,list) else x.tanh()\n",
    "    def __repr__(self):\n",
    "        return \"Tanh\"\n",
    "    def parameters(self):\n",
    "        return []\n",
    "# class SoftMax(Module):\n",
    "#     def __call__(self, x):\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self):\n",
    "        self.sequential = [\n",
    "            Linear(3,4),\n",
    "            Tanh(),\n",
    "            Linear(4,4),\n",
    "            Tanh(),\n",
    "            Linear(4,1),\n",
    "            Tanh()\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = [list(map(Var, r)) for r in x] if isinstance(x,list) else Var(x)\n",
    "        preds = []\n",
    "        for i in x:\n",
    "            for layer in self.sequential:\n",
    "               i = layer(i)\n",
    "            preds.append(i)\n",
    "        return preds\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.sequential for p in layer.parameters()]"
   ],
   "id": "90c4039bc2c791a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def MSELoss(ypred, ytrue):\n",
    "    return sum((yp - yt)**2 for yp, yt in zip(ypred, ytrue)) / len(ypred)\n",
    "def BCELoss(ypred, ytrue):\n",
    "    total_loss = 0\n",
    "    n = len(ypred)\n",
    "\n",
    "    for yp, yt in zip(ypred, ytrue):\n",
    "        yp = Var(min(max(yp.x, 1e-7), 1 - 1e-7))\n",
    "        total_loss += yt * yp.log() - (Var(1) - yt) * (Var(1) - yp).log()\n",
    "    return -total_loss / n\n",
    "def MaxMarginLoss(ypred, ytrue):\n",
    "    return sum([(1 + -yt*yp).relu() for yt, yp in zip(ytrue, ypred)]) / len(ypred)\n",
    "def L2Regularization(params, alpha = 1e-4):\n",
    "    return alpha * sum((p*p for p in params))\n",
    "class SGD:\n",
    "    def __init__(self,params, lr):\n",
    "        self.learning_rate = lr\n",
    "        self.params = params\n",
    "    def step(self, schedule=None):\n",
    "        if schedule:\n",
    "            self.learning_rate *= schedule\n",
    "        for p in self.params:\n",
    "            p.x += -self.learning_rate * p.grad\n",
    "\n"
   ],
   "id": "f73bc649fc8c13fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "y = [1.0, -1.0, -1.0, 1.0]"
   ],
   "id": "dcc0ab782907e543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "model = MLP()\n",
    "criterion = SGD(model.parameters(), 0.1)\n",
    "for k in range(20):\n",
    "\n",
    "    ypred = model(X)\n",
    "    loss = MSELoss(ypred, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    criterion.step()\n",
    "\n",
    "    print(k, loss.x)"
   ],
   "id": "dec21c7ad8e630b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "y = y*2 - 1\n",
    "\n",
    "class Moon_MLP(Module):\n",
    "    def __init__(self):\n",
    "        self.sequential = [\n",
    "            Linear(2,16),\n",
    "            ReLu(),\n",
    "            Linear(16,16),\n",
    "            ReLu(),\n",
    "            Linear(16,1)\n",
    "        ]\n",
    "    def __call__(self, x):\n",
    "        x = [list(map(Var, r)) for r in x] if isinstance(x,list) else Var(x)\n",
    "        preds = []\n",
    "        for i in x:\n",
    "            for layer in self.sequential:\n",
    "               i = layer(i)\n",
    "            preds.append(i)\n",
    "        return preds\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.sequential)}]\"\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.sequential for p in layer.parameters()]\n",
    "\n",
    "model = Moon_MLP()\n",
    "criterion = SGD(model.parameters(), 1)\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "\n",
    "    preds = model(X.tolist())\n",
    "    loss = MaxMarginLoss(preds, y) + L2Regularization(model.parameters())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    criterion.step(math.exp(-0.001*epoch))\n",
    "\n",
    "    accuracy = [(yi > 0) == (scorei.x > 0) for yi, scorei in zip(y, preds)]\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "    print(epoch, loss, f'{acc*100}%')\n"
   ],
   "id": "15e9a8e939d18721",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "def loss(batch_size=None):\n",
    "\n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    #else:\n",
    "     #   ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "     #   Xb, yb = X[ri], y[ri]\n",
    "\n",
    "    # forward the model to get scores\n",
    "    scores = model(Xb.tolist())\n",
    "\n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.x > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)\n",
    "model = Moon_MLP()\n",
    "criterion = SGD(model.parameters(), 1)\n",
    "for k in range(100):\n",
    "\n",
    "    # forward\n",
    "    total_loss, acc = loss()\n",
    "\n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "\n",
    "    criterion.step(math.exp(-0.001*k))\n",
    "\n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.x}, accuracy {acc*100}%\")"
   ],
   "id": "869a47f9c1a9fe27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize decision boundary\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "scores = model(Xmesh.tolist())\n",
    "Z = np.array([s.x > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ],
   "id": "b196f0f4ef0f1fb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "california = fetch_california_housing(download_if_missing=True)\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class CH_MLP(Module):\n",
    "    def __init__(self):\n",
    "        self.sequential = [\n",
    "            Linear(13,64),\n",
    "            ReLu(),\n",
    "            Linear(64,64),\n",
    "            ReLu(),\n",
    "            Linear(64,1)\n",
    "        ]\n",
    "    def __call__(self, x):\n",
    "        x = [list(map(Var, r)) for r in x] if isinstance(x,list) else Var(x)\n",
    "        preds = []\n",
    "        for i in x:\n",
    "            for layer in self.sequential:\n",
    "               i = layer(i)\n",
    "            preds.append(i)\n",
    "        return preds\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.sequential)}]\"\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.sequential for p in layer.parameters()]\n",
    "\n",
    "model = CH_MLP()\n",
    "criterion = SGD(model.parameters(), 1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    preds = model(X_train.tolist())\n",
    "    loss = MSELoss(preds, y_train) + L2Regularization(model.parameters())\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    criterion.step(math.exp(-0.001*epoch))\n",
    "    print(epoch, loss)"
   ],
   "id": "86fbd5c2a2af1a4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test the model\n",
    "preds = model(X_test.tolist())\n",
    "test_loss = MSELoss(preds, y_test)\n",
    "print(f\"Test loss: {test_loss.x}\")"
   ],
   "id": "d78da796a028d82d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
